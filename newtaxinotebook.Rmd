---
title: "New York Taxi Time Predictions"
output: html_notebook
---
#By : Rishit Dholakia



This is a challenge in which the new York taxi trip duration is to be predicted by the user . This is a form of *predictive modelling*. It  requires the following *feature engineering, data cleaning , analysis of the data, training and prediction*  

##Importing Dataset

We first import the dataset for the training and test consisting of 14 lakh and 6 lakh respectively

```{r Importing of the dataset}
library(data.table)
train <- fread("train.csv")
test_csv <- fread("test.csv")
head(train)
summary(train)
```



TO BE NOTED : Before we get into feature engineering we need to first clear up the existing data and make the features much more useful to view . So that we can apply the preprocessing techniques .



##Concatenation 

In this we concatenate the training and the test set as if any of the changes are applied in the features it will be affected  both in one go.
We notice that there is a difference in the number of columns in training and test and those are adjusted in the following code.

```{r}
col_train <- colnames(train)
col_test <- colnames(test_csv)
col_train[!col_train %in% col_test]
train$dropoff_datetime <- NULL
test.trip_duration <- data.frame(test_csv[],trip_duration = rep("None",nrow(test_csv)))
datanew <- rbind(train,test.trip_duration)
str(datanew)
```

Visualizing the format of the date
```{r}
head(datanew$pickup_datetime)

```

We now pre-process this by splitting the date and the time. 

```{r}
datanew$pickup_datetime <- as.character(datanew$pickup_datetime)
library(stringr) 
split_DT <- as.data.frame(str_split_fixed(datanew$pickup_datetime," ",2))
datanew1<-cbind(datanew,split_DT)
head(datanew1)
```

#Renaming and adding of the hour columns 

Now that we have names of the new columns as V1 and V2 we have to rename them.
there is addition of the hour column also in this. To make it simplify the complexity of the code. 
```{r}
library(data.table)
setnames(datanew1,"V1","pickup_date")
setnames(datanew1,"V2","pickup_time")
datanew1$pickup_date<-as.character(datanew1$pickup_date)
datanew1$pickup_time<-as.character(datanew1$pickup_time)
str(datanew1)

```


```{r}
datanew1$pickup_date<-as.Date(datanew1$pickup_date)
time_split <- as.data.frame(str_split_fixed(datanew1$pickup_time,":",3))

time_hr <-time_split[,c("V1")]

time_hr <- as.data.frame(time_hr)
datanew2<-cbind(datanew1,time_hr)
datanew2$time_hr<-as.factor(datanew2$time_hr)
datanew2$pickup_datetime <- NULL
head(datanew2)
summary(datanew2)
```

# Training benchmark on the current features and testing the working of the model

Before we add additional features , the benchmark of the RMSE value is going to be used in order to get an idea on the prediction that is need for the further analysis of the feature .Since the dataset is large , artificial neural networks is a good algorithm  to train the models.the H20.ai api is used in order to cluster up the data for faster processing.


```{r}
library(h2o)
h2o.init(nthreads = 3)
datanew2$pickup_time <- NULL
datanew2$trip_duration=as.numeric(datanew2$trip_duration)
train.h20<-as.h2o(datanew2[1:1458644,])
train.h20$vendor_id<-as.factor(train.h20$vendor_id)
train.h20$store_and_fwd_flag<-as.factor(train.h20$store_and_fwd_flag)

model = h2o.deeplearning(x=c(2,3,4,5,6,7,8),y = 9,
                         training_frame = as.h2o(train.h20),
                         activation = 'Rectifier',
                         hidden = c(100,100),
                         epochs = 100
                         )
```

```{r}
print(model)
```

Since the value of RMSLE value is more than 1 . There is a need for feature engineering and analysis. 

Soo lets head on to that !!!!!!!!!!

##Feature Engineering and Data cleaning based on the analysis of data

first we analysis the

```{r}
library(ggplot2)
a<-as.data.frame(datanew2)
ggplot(a,aes(x=as.factor(time_hr),fill="Freq"))+geom_bar()+xlab("Hour")+ylab("Number of Passengers")+labs(fill="Travelled")

```
It can be noticed that the number of people traveliling at 4 in the night is of least frequency and that htere is no abnormal situation in this features.


#Add a new feature of the sessions of a day 

The interesting fact that we can do based on the visualization above is that we can break the day into morning,evening,night etc based on the hours of the day !!!!! Lets impliment this piece of code . 


Categorizing time into early morning, morning, afternoon, evening, night, late night
 03-07 :  Early morning, 08-11 : Morning, 12-15 : Afternoon, 16-19 : Evening, 20-22 , Night, 
 23-02: Late Night

```{r}
library(data.table)

time_day <- as.data.frame(as.numeric(datanew2$time_hr))
setnames(time_day,"as.numeric(datanew2$time_hr)","time_hrs")
time_mod2 <- data.frame(time_day= rep("None",nrow(datanew2)))
time_mod2$time_day <- as.character(time_mod2$time_day)


time_mod2$time_day[which(time_day$time_hr>=8 & time_day$time_hr <=11)] <- as.character("M")
time_mod2$time_day[which(time_day$time_hr>=12 & time_day$time_hr <=15)] <- as.character("A")
time_mod2$time_day[which(time_day$time_hr>=16 & time_day$time_hr <=19)] <- as.character("E")
time_mod2$time_day[which(time_day$time_hr>=20 & time_day$time_hr <=22)] <- as.character("N")
time_mod2$time_day[which(time_day$time_hr>=23)] <- as.character("LN")
time_mod2$time_day[which(time_day$time_hr <=2)] <- as.character("LN")
time_mod2$time_day[which(time_day$time_hr>=3 & time_day$time_hr <=7)] <- as.character("EM")


datanew2<-cbind(datanew2,time_mod2)
datanew2$time_hr<-NULL

```

We view it and check the summary of this piece of code. 
*note*:- we can remove the hours column as the sessions feature has helped in the better resolution and understanding by the program . 


```{r}
summary(datanew2)
```

```{r}
head(datanew2)
```


#Scaling of the data

Now this is one important criteria as this will help in gradient decent effectively.

```{r}
datanew2$passenger_count<-scale(datanew2$passenger_count)
datanew2$pickup_longitude<-scale(datanew2$pickup_longitude)
datanew2$pickup_latitude<-scale(datanew2$pickup_latitude)
datanew2$dropoff_longitude<-scale(datanew2$dropoff_longitude)
datanew2$dropoff_latitude<-scale(datanew2$dropoff_latitude)
head(datanew2)
```


#Adding distance feature column 

This is the most important feature as it playes a major role in this feture engineering .
We do this using a library called geosphere. 

```{r}
library(geosphere)
datanew2$distance <- distHaversine(datanew1[,5:6], datanew1[,7:8])/1000

summary(datanew2$distance[which(datanew2$trip_duration<5)])
library(ggplot2)
summary(datanew2$trip_duration)
summary(datanew2$distance)
```

Now some things see very wierd in this as we all know distance cannot be 0 and that max distance of 1242 km thats like allooott !!!! Who would sit that long in a car that too in a cab . So the best thing to do is visualize this and with the elp of it remove the anomalies. 


```{r}
dt<-ggplot() +
  geom_point(aes(x = datanew2$trip_duration, y = datanew2$distance),
             colour = 'red') +xlab("Trip Duration")+ylab("Distance")+labs(fill="Travelled") 

dt + coord_cartesian(ylim= c(0,400))
```

From this we can visualize that some the cars travelled even 1000 kms in 1000 seconds !! thats ot even possible . And that these anamolies are eliminated by keeping a resonable distance travelled as distace less that 45 in the graph. 

Making a new dataframe

```{r}
temp<-datanew2[-(1:nrow(train))]
temp2<-datanew2[1:nrow(train)]
temp2<-temp2[which(temp2$distance<45)]
head(temp2)
```

```{r}
summary(temp2$distance)
```

We need to remove the 0 distance also and that is important,It is taken for a value that is greater than the zero .

```{r}
temp2<-temp2[which(temp2$distance>0)]
```

```{r}
summary(temp2$distance)
```


#Adding the velocity feature 

with the help of distance and time we calculate the velocity feature abd this will be per hour that is why  we multiply with 3600

```{r}
velocity <- (temp2[1:1452623,]$distance/temp2[1:1452623,]$trip_duration)
velocity <- velocity*3600
temp2$velocity <- velocity
```

```{r}
summary(temp2$velocity)
```

As usual we have anomalies in the solution and that we have the fact we have to follow the rules and regulations of the of the NEW YORK driving authority i.e knowing the speed limit of the cabs in new york this will help remove the anomulous limits .

According to the driving authority the speed limit is 55 miles per hour converting it kms it is 90 kms per hour and minimum speed limit is considered to be 5 kms per hr.


```{r}
temp2<-temp2[which(temp2$velocity < 90)]
temp2<-temp2[!(temp2$velocity <5)]
head(temp2)
```

```{r}
summary(temp2$velocity)
```
#Considering the practical timings 

Now that we have a reasonable velocity , a check on the time must also be done , for a distance of 44 kilometers and going with a speed of 10 kms/hr we are likly to get a time of 1.46 hors that is close to 2 hours which is 7200 seconds and as a matter of fact no cab journey can be less that 60 seconds i.e 1 min
so now we remove the anomolus data by keeping a limit of 7200 seconds and 60 sec.


```{r}
temp2<-temp2[which(temp2$trip_duration > 60)]
temp2<-temp2[which(temp2$trip_duration < 7200)]
summary(temp2$trip_duration)
```


##Additional data (WEATHER DATA)

Since this is a playground challange , extra data is allowed to be added and so one of the best data that can be added is the temperature . As temperature can also affect the trip durtion in this it is likly to be a feature in this .
I got this dataset from the kaggle website , it is the newyork 2016 dataset.

```{r}
library(readr)
weather <- read_csv("weatherdatanew1.csv")
head(weather)
```

```{r}
summary(weather)
```

```{r}
str(temp2$pickup_date)
temp2$pickup_date<-as.character(temp2$pickup_date)
head(temp2)

```

Adding a rain feature 

```{r}
rain = as.numeric(ifelse(weather$precipitation == "T", "0.01", weather$precipitation))
```

```{r}
weather$rain<-rain
weather$s_fall<-NULL
weather$allprecp<- NULL
weather$has_snow<- NULL
weather$has_rain<- NULL
weather$precipitation<-NULL
weather$`snow fall`<-NULL
weather$`snow depth`<-NULL
```

```{r}
head(weather)
```

#Merging 
Here we merge the two datasets , stemp2 and weather 

```{r}

weather$date<-as.character(weather$date)
setnames(temp2,"pickup_date","date")
temp2$date<-as.character(temp2$date)
temp2 <- merge(temp2, weather, by = "date")
temp2$pickup_date<-NULL
temp2$`snow fall`<-NULL
temp2$`snow depth`<-NULL
temp2$precipitation<-NULL
head(temp2)
```

##Correlation of the features for removing predictors 
 
This is one important preprocessing tool that helps the to know which all of the following columns are correlated . if they are closly correlated then we can use PCA (principle component analysis) on those features and then remove them from the and add up the pca array to the column in the temp2 dataframe .


since it works only for the numeric features we consider those .
```{r}
cntemp<-temp2
library(microbenchmark)
cntemp2<-Filter(is.numeric, cntemp)
cntemp2$`maximum temperature`<-NULL
cntemp2$`minimum temperature`<-NULL

cor(cntemp2)
M<-cor(cntemp2)

```

We now plot the correlation .
```{r}

library(corrplot)
corrplot(M, method = "circle")
```

To get a better idea lets do it with numbers rather than any other graph.

```{r}
corrplot(M, method = "number")
```

##Training the dataset 

As the above I will be using ANN for the predictions .The activation function that I will be using is rectifier  .
As its derivative value is 1 . gradient decent on this is much effective .


```{r}
str(temp2)
```

##Splitting of the dataset into taining and validation set

```{r}
library(caTools)
set.seed(123)
split = sample.split(temp2$trip_duration, SplitRatio = 0.8)
training_set = subset(temp2, split == TRUE)
test_set = subset(temp2, split == FALSE)
```


Trying with 100 nodes in 2 layers 

First fixing the bias i.e the RMSE value that we have and get it as loer as possible for the training set and then move on to the test set and check on that .
Usually the best ranges are between 60-128 nodes in case of this dataset.

```{r}
library(h2o)
h2o.init(nthreads = 3)
training_set$date<- as.Date(training_set$date)
train.h20<-as.h2o(training_set)
train.h20$vendor_id<-as.factor(train.h20$vendor_id)
train.h20$store_and_fwd_flag<-as.factor(train.h20$store_and_fwd_flag)
train.h20$time_day<-as.factor(train.h20$time_day)
model2 = h2o.deeplearning(x=c(3,4,5,6,7,8,9,11,12,13,16,17),y = 10,
                         training_frame = as.h2o(train.h20),
                         activation = 'Rectifier',
                         hidden = c(100,100),
                         epochs = 100
)
```

```{r}
print(model2)
```

Trying 60 nodes in each layer 

```{r}
library(h2o)
h2o.init(nthreads = 3)
training_set$date<- as.Date(training_set$date)
train.h20<-as.h2o(training_set)
train.h20$vendor_id<-as.factor(train.h20$vendor_id)
train.h20$store_and_fwd_flag<-as.factor(train.h20$store_and_fwd_flag)
train.h20$time_day<-as.factor(train.h20$time_day)
model = h2o.deeplearning(x=c(3,4,5,6,7,8,9,11,12,13,16,17),y = 10,
                         training_frame = as.h2o(train.h20),
                         activation = 'Rectifier',
                         hidden = c(90,90),
                         epochs = 100
)
```


```{r}
print(model)
```


Trying with 110 nodes in each layer 



```{r}
library(h2o)
h2o.init(nthreads = 3)
training_set$date<- as.Date(training_set$date)
train.h20<-as.h2o(training_set)
train.h20$vendor_id<-as.factor(train.h20$vendor_id)
train.h20$store_and_fwd_flag<-as.factor(train.h20$store_and_fwd_flag)
train.h20$time_day<-as.factor(train.h20$time_day)
model = h2o.deeplearning(x=c(3,4,5,6,7,8,9,11,12,13,16,17),y = 10,
                         training_frame = as.h2o(train.h20),
                         activation = 'Rectifier',
                         hidden = c(110,110),
                         epochs = 100
                      
)
```

```{r}
print(model)
```

now that we have go a reaaly close rmse value of 5.8 we can move on to the test set to check on the varience and see if the RMSE value resembles more or less.

#Predicting the validation set

```{r}
h2o.init(nthreads = 3)
test_set$trip_duration=as.numeric(test_set$trip_duration)
test.h20<-as.h2o(test_set)
test.h20$vendor_id<-as.factor(test.h20$vendor_id)
test.h20$store_and_fwd_flag<-as.factor(test.h20$store_and_fwd_flag)

test.h20$time_day<-as.factor(test.h20$time_day)


y_pred = h2o.predict(model2, newdata = as.h2o(test.h20))
y_pred= as.data.frame(y_pred)
```



```{r}
library(hydroGOF)
rmse(y_pred,test_set$trip_duration)
```

#Variable importance in the graph 

This is for the  best model i obtained till now .
```{r}
library(h2o)
localH2O<-h2o.init(nthreads = 3)


h2o.varimp_plot(model2)

```

This is the variable plot for the second best model that i obtained till now .
```{r}
library(h2o)
localH2O<-h2o.init(nthreads = 3)


h2o.varimp_plot(model)

```






##Final result of the ANN

With a rmse value of 5.99 and the variance value of 5.95 it is pretty fair enough to say it is a sustainable model. There was a drastic reduction from a value of 698 as seen above in the benchmark section to 5.8 after apply the preprocessing feature engineering and the right fit deep learning algorithm as compared to random forest which would be less effective on this dataset .

An achievement of not over fitting .

#RMSE on training set     = 5.98
#RMSE on validation set   = 5.95

------------------------------------------------------THANK YOU for viewing----------------------------------------------------- 



```{r}









